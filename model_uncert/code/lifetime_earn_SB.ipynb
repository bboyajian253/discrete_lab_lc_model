{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running main\n",
      "*****Running main_io with default out_folder_name*****\n",
      "Solver ran in 7.288547200005269 seconds\n",
      "Calibrating with alpha_lab_targ = 0.33497447, w0_mean_targ = 2.1988928, w0_sd_targ = 0.29847395, \n",
      "                                        w1_targ = 0.2928040000000003, w2_targ = 0.2746706000000003, wH_targ = 0.051316846, phi_H_targ = 0.053118114,\n",
      "                                        dpi_BB_targ = 0.50886095, dpi_GG_targ = 0.34358001, eps_gg_targ = 0.39124098057692314\n",
      "***** Calibration iteration 0 *****\n",
      "Calibrating epsilon_gg\n",
      "Calibrating w0_mu\n",
      "Calibrating w0_sigma\n",
      "***** Calibration iteration 1 *****\n",
      "Calibrating w0_mu\n",
      "Calibrating w0_sigma\n",
      "***** Calibration iteration 2 *****\n",
      "Calibrating w0_mu\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 3 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 4 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 5 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 6 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 7 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 8 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 9 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "Calibrating wH\n",
      "***** Calibration iteration 10 *****\n",
      "Calibrating w0_mu\n",
      "Calibrating w0_sigma\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 11 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "***** Calibration iteration 12 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "Calibrating wH\n",
      "Calibrating alpha\n",
      "Calibration converged after 13 iterations\n",
      "********** delta_pi_BB and delta_pi_GG calibration was skipped **********\n",
      "********** phi_H calibration was skipped **********\n",
      "epsilon_gg = 0.05, epsilon_gg mom = 0.39143552419645516, epsilon_gg mom targ = 0.39124098057692314\n",
      "w0_weights = [0.08450826 0.22388249 0.26234589 0.19741129 0.11709157 0.06081168\n",
      " 0.029296   0.01353685 0.00612215 0.00499381], w0_mean = 2.1985018305496316, w0_mean_targ = 2.1988928\n",
      "w0_sd = 0.29910647111093475, w0_sd_targ = 0.29847395\n",
      "w1 = 0.0229644775390625, w1 moment = 0.2927013102624807, w1 mom targ = 0.2928040000000003\n",
      "w2 = -0.000457763671875, w2 moment = 0.27481516815493867, w2 mom targ = 0.2746706000000003\n",
      "wH = 0.06103515625, wH moment = 0.052261883406226506, wH mom targ = 0.051316846\n",
      "alpha = 0.3789124609375, alpha moment = 0.3344272054005117, alpha mom targ = 0.33497447\n",
      "Calibration ran in 126.72454670001753 seconds\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "import plot_inequality as plot_ineq\n",
    "import time\n",
    "import importlib\n",
    "import io_manager\n",
    "import numpy as np\n",
    "importlib.reload(plot_ineq)\n",
    "#run stuff here\n",
    "start_time = time.perf_counter()\n",
    "print(\"Running main\")\n",
    "\n",
    "of_name = None\n",
    "main_path = \"C:/Users/Ben/My Drive/PhD/PhD Year 3/3rd Year Paper/Model/My Code/MH_Model/my_code/model_uncert/\"\n",
    "input_path = main_path + \"input/50p_age_moms/\"\n",
    "trans_path_uncond = input_path + \"MH_trans_uncond_age.csv\"\n",
    "\n",
    "trans_path_50p = input_path + \"MH_trans_by_MH_clust_age.csv\"\n",
    "type_path_50p = input_path + \"MH_clust_50p_age_pop_shares.csv\"\n",
    "\n",
    "do_dpi_calib = False\n",
    "output_flag = False\n",
    "myPars, myShocks, sols, sims = main.main_io(main_path, out_folder_name = of_name, \n",
    "                                            H_trans_uncond_path = trans_path_uncond, \n",
    "                                            H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p,\n",
    "                                            output_flag = output_flag, do_dpi_calib = do_dpi_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre - np.mean(H_hist): 1.0\n",
      "*****Running main_io with default out_folder_name*****\n",
      "Solver ran in 0.6511146000120789 seconds\n",
      "Calibrating with alpha_lab_targ = 0.33497447, w0_mean_targ = 2.1988928, w0_sd_targ = 0.29847395, \n",
      "                                        w1_targ = 0.2928040000000003, w2_targ = 0.2746706000000003, wH_targ = 0.051316846, phi_H_targ = 0.053118114,\n",
      "                                        dpi_BB_targ = 0.50886095, dpi_GG_targ = 0.34358001, eps_gg_targ = 0.39124098057692314\n",
      "***** Calibration iteration 0 *****\n",
      "Calibrating w1\n",
      "Calibrating w2\n",
      "Calibrating alpha\n",
      "Calibration converged after 1 iterations\n",
      "********** delta_pi_BB and delta_pi_GG calibration was skipped **********\n",
      "********** wH calibration was skipped **********\n",
      "********** phi_H calibration was skipped **********\n",
      "********** epsilon_gg calibration was skipped ********\n",
      "epsilon_gg = 0.05, epsilon_gg mom = 0.3914150339207644, epsilon_gg mom targ = 0.39124098057692314\n",
      "w0_weights = [0.10380345 0.24244015 0.26218454 0.18638374 0.10606517 0.05340992\n",
      " 0.02513455 0.01140719 0.005088   0.00408328], w0_mean = 2.1992138931317724, w0_mean_targ = 2.1988928\n",
      "w0_sd = 0.2988256606885218, w0_sd_targ = 0.29847395\n",
      "w1 = 0.0231170654296875, w1 moment = 0.2936124713551056, w1 mom targ = 0.2928040000000003\n",
      "w2 = -0.00045490264892578125, w2 moment = 0.27501582274278924, w2 mom targ = 0.2746706000000003\n",
      "wH = 0.06103515625, wH moment = -999.999, wH mom targ = 0.051316846\n",
      "alpha = 0.38281867187499996, alpha moment = 0.3346921644300321, alpha mom targ = 0.33497447\n",
      "Calibration ran in 34.56340730001102 seconds\n",
      "Post - np.mean(H_hist): 1.0\n"
     ]
    }
   ],
   "source": [
    "from pars_shocks import Pars, Shocks\n",
    "import pars_shocks\n",
    "import calibration as calib\n",
    "importlib.reload(main)\n",
    "importlib.reload(calib)\n",
    "\n",
    "# calib phi_H: time cost counterfactual\n",
    "# myPars_tc, myShocks_tc, sols_tc, sims_tc = main.main_io(main_path, out_folder_name = of_name,\n",
    "#                                                         H_trans_uncond_path = trans_path_uncond, \n",
    "#                                                         H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p,\n",
    "#                                                         output_flag = False, do_dpi_calib = False, do_phi_H_calib=True) \n",
    "# H = good counterfactual\n",
    "# myPars_H_good: Pars = main.pars_factory(main_path, \n",
    "#                                         H_trans_uncond_path = trans_path_uncond, \n",
    "#                                         H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p)\n",
    "myPars_H_good = myPars\n",
    "myShocks_H_good: Shocks = Shocks(myPars_H_good)\n",
    "\n",
    "H_hist = np.ones(myPars.state_space_shape_sims, dtype=np.int64)\n",
    "myShocks_H_good.H_hist = H_hist\n",
    "print(\"Pre - np.mean(H_hist):\", np.mean(H_hist))\n",
    "\n",
    "myPars_H_good, myShocks_H_good, sols_H_good, sims_H_good = main.main_io(main_path, out_folder_name = of_name, \n",
    "                                                                    H_trans_uncond_path = trans_path_uncond, \n",
    "                                                                    H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p,\n",
    "                                                                    myPars = myPars_H_good, myShocks=myShocks_H_good, \n",
    "                                                                    calib_flag=True, sim_no_calib=False, output_flag = False, \n",
    "                                                                    do_wH_calib = False, do_phi_H_calib=False, \n",
    "                                                                    do_dpi_calib = False, do_eps_gg_calib=False)\n",
    "\n",
    "H_hist = myShocks_H_good.H_hist\n",
    "print(\"Post - np.mean(H_hist):\", np.mean(H_hist))\n",
    "# # wH = 0 counterfactual\n",
    "# myPars_no_wH: Pars = main.pars_factory(main_path,\n",
    "#                                         H_trans_uncond_path = trans_path_uncond, \n",
    "#                                         H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p)\n",
    "# myPars_no_wH.wH_coeff = 0.0\n",
    "# myPars_no_wH, myShocks_no_wH, sols_no_wH, sims_no_wH = main.main_io(main_path, out_folder_name = of_name,\n",
    "#                                                                     H_trans_uncond_path = trans_path_uncond, \n",
    "#                                                                     H_trans_path = trans_path_50p, H_type_pop_share_path = type_path_50p,\n",
    "#                                                                     myPars = myPars_no_wH, myShocks=myShocks, \n",
    "#                                                                     output_flag = False, do_wH_calib = False, do_dpi_calib = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_uncert as model\n",
    "from typing import Tuple\n",
    "age_25_ind = np.where(myPars.age_grid == 25)[0][0] \n",
    "age_55_ind = np.where(myPars.age_grid == 55)[0][0]\n",
    "\n",
    "lab_earn_trim: np.array = sims[\"lab_earnings\"][:, :, :, age_25_ind:age_55_ind+1]*12\n",
    "# lab_earn_trim_no_wH: np.array = sims_no_wH[\"lab_earnings\"][:, :, :, age_25_ind:age_55_ind+1]*12\n",
    "lab_earn_trim_H_good: np.array = sims_H_good[\"lab_earnings\"][:, :, :, age_25_ind:age_55_ind+1]*12\n",
    "# lab_earn_trim_tc: np.array = sims_tc[\"lab_earnings\"][:, :, :, age_25_ind:age_55_ind+1]*12\n",
    "\n",
    "log_lab_earn = np.log(lab_earn_trim)\n",
    "# log_lab_earn_no_wH = np.log(lab_earn_trim_no_wH)\n",
    "log_lab_earn_H_good = np.log(lab_earn_trim_H_good)\n",
    "# log_lab_earn_tc = np.log(lab_earn_trim_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calc_wstats(earnings: np.array, weights_lab_fe: np.array, weight_H_type: np.array) -> dict:\n",
    "\n",
    "    weights = np.outer(weights_lab_fe, weight_H_type)\n",
    "    sim_num = earnings.shape[2]\n",
    "    age_num = earnings.shape[3]\n",
    "    weights_exp = np.repeat(weights[:, :, np.newaxis], sim_num, axis=2)\n",
    "    weights_exp = np.repeat(weights_exp[:, :, :, np.newaxis], age_num, axis=3)\n",
    "\n",
    "    flat_earns = earnings.flatten()\n",
    "    flat_weights = weights_exp.flatten()\n",
    "\n",
    "    mean_earn = np.average(flat_earns, weights=flat_weights)\n",
    "\n",
    "    #calculate the weighted median\n",
    "    sorted_inds = np.argsort(flat_earns)\n",
    "    sorted_earns = flat_earns[sorted_inds]\n",
    "    sorted_weights = flat_weights[sorted_inds]\n",
    "    cum_weights = np.cumsum(sorted_weights)\n",
    "    cutoff = 0.5 * cum_weights[-1]\n",
    "    median_earn = sorted_earns[np.searchsorted(cum_weights, cutoff)]\n",
    "\n",
    "    #calc weighted variance\n",
    "    var_earn = np.average((flat_earns - mean_earn)**2, weights=flat_weights)\n",
    "    #calc weighted std dev\n",
    "    std_earn = np.sqrt(var_earn)\n",
    "\n",
    "    #calc weighted min and max\n",
    "    min_earn = np.min(flat_earns)\n",
    "    max_earn = np.max(flat_earns)\n",
    "\n",
    "    # Calculate weighted percentiles\n",
    "    pct_10 = np.percentile(sorted_earns, 10)\n",
    "    pct_25 = np.percentile(sorted_earns, 25)\n",
    "    pct_75 = np.percentile(sorted_earns, 75)\n",
    "    pct_90 = np.percentile(sorted_earns, 90)\n",
    "\n",
    "    # Step 5: Return the results in a dictionary\n",
    "    ret_dict = {\n",
    "        \"mean\": mean_earn,\n",
    "        \"median\": median_earn,\n",
    "        \"var\": var_earn,\n",
    "        \"std_dev\": std_earn,\n",
    "        \"min\": min_earn,\n",
    "        \"max\": max_earn,\n",
    "        \"P10\": pct_10,\n",
    "        \"P25\": pct_25,\n",
    "        \"P75\": pct_75,\n",
    "        \"P90\": pct_90\n",
    "    }\n",
    "\n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_weighted_lt_stats(earnings: np.array, weights_lab_fe: np.array, weights_H_type: np.array) -> dict:\n",
    "    # Step 1: Calculate lifetime earnings (annualized mean earnings from ages 25-55)\n",
    "    # Average across the last axis (age), assuming the earnings matrix is already restricted to ages 25-55.\n",
    "    lifetime_earnings = earnings.mean(axis=-1)  # Shape: (lab_fe, H_type, simulation_number)\n",
    "    \n",
    "    # Step 2: Expand the weights to match the dimensions of lifetime_earnings\n",
    "    # Create a grid of weights by multiplying the lab_fe and H_type weights\n",
    "    weights = np.outer(weights_lab_fe, weights_H_type)  # Shape: (lab_fe, H_type)\n",
    "    \n",
    "    # Reshape the weights to broadcast over the (simulation_number) dimension\n",
    "    sim_num = lifetime_earnings.shape[2]\n",
    "    weights_expanded = np.repeat(weights[:, :, np.newaxis], sim_num, axis=2)  # Shape: (lab_fe, H_type, simulation_number)\n",
    "\n",
    "    # Step 3: Flatten the earnings and weights for weighted calculation\n",
    "    flattened_earnings = lifetime_earnings.flatten()  # Shape: (lab_fe * H_type * simulation_number,)\n",
    "    flattened_weights = weights_expanded.flatten()    # Shape: (lab_fe * H_type * simulation_number,)\n",
    "\n",
    "    # Step 4: Compute weighted statistics\n",
    "\n",
    "    # Calculate weighted mean\n",
    "    mean_earn = np.average(flattened_earnings, weights=flattened_weights)\n",
    "\n",
    "    # Calculate weighted median\n",
    "    sorted_indices = np.argsort(flattened_earnings)\n",
    "    sorted_earnings = flattened_earnings[sorted_indices]\n",
    "    sorted_weights = flattened_weights[sorted_indices]\n",
    "    cumsum_weights = np.cumsum(sorted_weights)\n",
    "    cutoff = 0.5 * cumsum_weights[-1]\n",
    "    median_earn = sorted_earnings[np.searchsorted(cumsum_weights, cutoff)]\n",
    "\n",
    "    # Calculate weighted variance\n",
    "    var_earn = np.average((flattened_earnings - mean_earn) ** 2, weights=flattened_weights)\n",
    "\n",
    "    # Calculate weighted standard deviation\n",
    "    std_earn = np.sqrt(var_earn)\n",
    "\n",
    "    # Calculate weighted min and max\n",
    "    min_earn = np.min(flattened_earnings)\n",
    "    max_earn = np.max(flattened_earnings)\n",
    "\n",
    "    # Calculate weighted percentiles\n",
    "    pct_10 = np.percentile(sorted_earnings, 10)\n",
    "    pct_25 = np.percentile(sorted_earnings, 25)\n",
    "    pct_75 = np.percentile(sorted_earnings, 75)\n",
    "    pct_90 = np.percentile(sorted_earnings, 90)\n",
    "\n",
    "    # Step 5: Return the results in a dictionary\n",
    "    ret_dict = {\n",
    "        \"mean\": mean_earn,\n",
    "        \"median\": median_earn,\n",
    "        \"var\": var_earn,\n",
    "        \"std_dev\": std_earn,\n",
    "        \"min\": min_earn,\n",
    "        \"max\": max_earn,\n",
    "        \"P10\": pct_10,\n",
    "        \"P25\": pct_25,\n",
    "        \"P75\": pct_75,\n",
    "        \"P90\": pct_90\n",
    "    }\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 18855.0831836132\n",
      "median 17912.706133770378\n",
      "var 34197778.60293625\n",
      "std_dev 5847.8866783596495\n",
      "min 10150.528096556149\n",
      "max 43856.00777400896\n",
      "P10 13237.80551742606\n",
      "P25 17833.567418027465\n",
      "P75 35615.96372111855\n",
      "P90 40228.34132994098\n",
      "\n",
      "mean 19895.38519846346\n",
      "median 18842.362212952954\n",
      "var 37919558.52957457\n",
      "std_dev 6157.8858814998\n",
      "min 11078.58211315283\n",
      "max 45221.668294151124\n",
      "P10 14424.840206312227\n",
      "P25 18747.847483198388\n",
      "P75 37117.63570232929\n",
      "P90 41739.30913706704\n"
     ]
    }
   ],
   "source": [
    "earnings = lab_earn_trim \n",
    "weights_lab_fe = myPars.lab_fe_weights\n",
    "weights_H_type = myPars.H_type_perm_weights\n",
    "lt_stats = calculate_weighted_lt_stats(earnings, weights_lab_fe, weights_H_type)\n",
    "for key in lt_stats:\n",
    "    print(key, lt_stats[key])\n",
    "\n",
    "print()\n",
    "\n",
    "# earnings_tc = lab_earn_trim_tc\n",
    "# weights_lab_fe = myPars_tc.lab_fe_weights\n",
    "# weights_H_type = myPars_tc.H_type_perm_weights\n",
    "# lt_stats_tc = calculate_weighted_lt_stats(earnings_tc, weights_lab_fe, weights_H_type)\n",
    "# for key in lt_stats_tc:\n",
    "    # print(key, lt_stats_tc[key])\n",
    "\n",
    "# print()\n",
    "\n",
    "# earnings_no_wH = lab_earn_trim_no_wH \n",
    "# weights_lab_fe = myPars_no_wH.lab_fe_weights\n",
    "# weights_H_type = myPars_no_wH.H_type_perm_weights\n",
    "# lt_stats_no_wH = calculate_weighted_lt_stats(earnings_no_wH, weights_lab_fe, weights_H_type)\n",
    "# for key in lt_stats_no_wH:\n",
    "#     print(key, lt_stats_no_wH[key])\n",
    "\n",
    "# print()\n",
    "\n",
    "earnings_H_good = lab_earn_trim_H_good\n",
    "weights_lab_fe = myPars_H_good.lab_fe_weights\n",
    "weights_H_type = myPars_H_good.H_type_perm_weights\n",
    "lt_stats_H_good = calculate_weighted_lt_stats(earnings_H_good, weights_lab_fe, weights_H_type)\n",
    "for key in lt_stats_H_good:\n",
    "    print(key, lt_stats_H_good[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_earn: 34197.78\n",
      "var_earn_H: 37919.56\n",
      "liftetime earn var %: -10.883104338007394\n",
      "log lifetime earn var %: -10.330634633798752\n"
     ]
    }
   ],
   "source": [
    "var_earn = lt_stats[\"var\"]\n",
    "var_earn_H = lt_stats_H_good[\"var\"]\n",
    "print(f\"var_earn: {round(var_earn/1000, 2)}\")\n",
    "print(f\"var_earn_H: {round(var_earn_H/1000, 2)}\")\n",
    "print(f\"liftetime earn var %: {(var_earn - var_earn_H)/var_earn*100}\")\n",
    "print(f\"log lifetime earn var %: {(np.log(var_earn) - np.log(var_earn_H))*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0    20274.967\n",
      "Name: mean, dtype: float64\n",
      "std_dev 0    10876.716\n",
      "Name: std_dev, dtype: float64\n",
      "P10 0    8689.2451\n",
      "Name: P10, dtype: float64\n",
      "P25 0    12836.752\n",
      "Name: P25, dtype: float64\n",
      "P50 0    18606.549\n",
      "Name: P50, dtype: float64\n",
      "P75 0    26088.908\n",
      "Name: P75, dtype: float64\n",
      "P90 0    34577.691\n",
      "Name: P90, dtype: float64\n",
      "PDF successfully created at C:/Users/Ben/My Drive/PhD/PhD Year 3/3rd Year Paper/Model/My Code/MH_Model/my_code/model_uncert/output/lt_earnings_stats_tc_counter.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import my_toolbox as tb\n",
    "outpath = myPars.path + \"output/\"\n",
    "\n",
    "lt_stats_data_path = input_path + \"lt_earn_stats.csv\" \n",
    "lt_stats_df =  pd.read_csv(lt_stats_data_path)\n",
    "# change the column names\n",
    "lt_stats_df.columns = [\"mean\", \"std_dev\",  \"P10\", \"P25\", \"P50\", \"P75\", \"P90\"]\n",
    "# convert to dictionary\n",
    "lt_stats_data = lt_stats_df.to_dict(orient = \"records\")[0]\n",
    "for key in lt_stats_df:\n",
    "    print(key, lt_stats_df[key])\n",
    "\n",
    "r2f =lambda x : round(x/1000, 2) \n",
    "r2 = lambda x : round(x, 2)\n",
    "rf = lambda x : round(x/1000)\n",
    "\n",
    "tab = [\n",
    "    \"\\\\documentclass[border=3mm,preview]{standalone}\",\n",
    "    \"\\\\usepackage{booktabs} \\n\",\n",
    "    \"\\\\usepackage{caption} \\n\",\n",
    "    \"\\\\usepackage{pdflscape} \\n\",\n",
    "    \"\\\\begin{document}\\n\",\n",
    "    \"\\\\begin{landscape}\\n\",\n",
    "    # \"\\\\textit{Liftime Earnings Statistics} \\\\\\\\ \\n\",\n",
    "    \"\\\\small\\n\",\n",
    "    \"\\\\begin{table} \\n\",\n",
    "    \"\\\\center\\\\caption*{Annualized Lifetime Earnings Statistics (1000s GBP)} \\n\",\n",
    "    \"\\\\begin{tabular}{l | l l l | l l l l l | l l l} \\n\",\n",
    "    \"\\\\toprule \\n\",\n",
    "    \" & \\\\multicolumn{3}{c}{Summary} & \\\\multicolumn{5}{c}{Percentiles} & \\\\multicolumn{3}{c}{Ratios} \\\\\\\\ \\n\",\n",
    "    # \"\\\\hline \\n\",\n",
    "    \"Source & Mean & Var. & SD & 10th & 25th & 50th & 75th & 90th & 90/10 & 90/50 & 50/10 \\\\\\\\ \\n\", \n",
    "    \"\\\\midrule \\n\",\n",
    "    f\"\"\"Baseline Model & {r2f(lt_stats['mean'])} & {rf(lt_stats['var'])} & {r2f(lt_stats['std_dev'])} & \n",
    "        {r2f(lt_stats['P10'])} & {r2f(lt_stats['P25'])} & {r2f(lt_stats['median'])} & {r2f(lt_stats['P75'])} & {r2f(lt_stats['P90'])} &\n",
    "        {r2(lt_stats['P90']/lt_stats['P10'])} & {r2(lt_stats['P90']/lt_stats['median'])} & {r2(lt_stats['median']/lt_stats['P10'])} \\\\\\\\ \\n\"\"\",\n",
    "    f\"\"\"$H = Good$ & {r2f(lt_stats_H_good['mean'])} & {rf(lt_stats_H_good['var'])} & {r2f(lt_stats_H_good['std_dev'])} & \n",
    "        {r2f(lt_stats_H_good['P10'])} & {r2f(lt_stats_H_good['P25'])} & {r2f(lt_stats_H_good['median'])} & {r2f(lt_stats_H_good['P75'])} & {r2f(lt_stats_H_good['P90'])} &\n",
    "        {r2(lt_stats_H_good['P90']/lt_stats_H_good['P10'])} & {r2(lt_stats_H_good['P90']/lt_stats_H_good['median'])} \n",
    "        & {r2(lt_stats_H_good['median']/lt_stats_H_good['P10'])} \\\\\\\\ \\n\"\"\",\n",
    "    # f\"\"\"$Time Cost$ & {r2f(lt_stats_tc['mean'])} & {rf(lt_stats_tc['var'])} & {r2f(lt_stats_tc['std_dev'])} & \n",
    "    #     {r2f(lt_stats_tc['P10'])} & {r2f(lt_stats_tc['P25'])} & {r2f(lt_stats_tc['median'])} & {r2f(lt_stats_tc['P75'])} & {r2f(lt_stats_tc['P90'])} &\n",
    "    #     {r2(lt_stats_tc['P90']/lt_stats_tc['P10'])} & {r2(lt_stats_tc['P90']/lt_stats_tc['median'])} \n",
    "    #     & {r2(lt_stats_tc['median']/lt_stats_tc['P10'])} \\\\\\\\ \\n\"\"\",\n",
    "    # f\"\"\"Data & {r2f(lt_stats_data['mean'])} & - & {r2f(lt_stats_data['std_dev'])} & \n",
    "    #     {r2f(lt_stats_data['P10'])} & {r2f(lt_stats_data['P25'])} & {r2f(lt_stats_data['P50'])} & {r2f(lt_stats_data['P75'])} & {r2f(lt_stats_data['P90'])} &\n",
    "    #     {r2(lt_stats_data['P90']/lt_stats_data['P10'])} & {r2(lt_stats_data['P90']/lt_stats_data['P50'])} & {r2(lt_stats_data['P50']/lt_stats_data['P10'])} \\\\\\\\ \\n\"\"\",\n",
    "    \"\\\\bottomrule \\n\",   \n",
    "    \"\\\\end{tabular}\\n\",\n",
    "    \"\\\\end{table}\\n\",\n",
    "    \"\\\\end{landscape}\\n\",\n",
    "    \"\\\\end{document}\\n\"\n",
    "]\n",
    "\n",
    "file_name = \"lt_earnings_stats_tc_counter.tex\"\n",
    "tb.list_to_tex(outpath, file_name, tab)\n",
    "tb.tex_to_pdf(outpath, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 9.768292289540378\n",
      "median 9.76220975258407\n",
      "var 0.0898328343083297\n",
      "std_dev 0.2997212610215193\n",
      "min 9.19287746997032\n",
      "max 10.661692356706128\n",
      "P10 9.457423141181915\n",
      "P25 9.756988361006\n",
      "P75 10.448386388200692\n",
      "P90 10.574838921139008\n",
      "\n",
      "mean 9.828472881913775\n",
      "median 9.82167840088289\n",
      "var 0.08946066588319593\n",
      "std_dev 0.29909975908247727\n",
      "min 9.28499425872583\n",
      "max 10.697147113991933\n",
      "P10 9.546493972393803\n",
      "P25 9.81521376793113\n",
      "P75 10.495431672568287\n",
      "P90 10.616251024066491\n"
     ]
    }
   ],
   "source": [
    "earnings = log_lab_earn \n",
    "weights_lab_fe = myPars.lab_fe_weights\n",
    "weights_H_type = myPars.H_type_perm_weights\n",
    "lt_stats = calculate_weighted_lt_stats(earnings, weights_lab_fe, weights_H_type)\n",
    "for key in lt_stats:\n",
    "    print(key, lt_stats[key])\n",
    "\n",
    "print()\n",
    "\n",
    "# earnings_no_wH = log_lab_earn_no_wH\n",
    "# weights_lab_fe = myPars_no_wH.lab_fe_weights\n",
    "# weights_H_type = myPars_no_wH.H_type_perm_weights\n",
    "# lt_stats_no_wH = calculate_weighted_lt_stats(earnings_no_wH, weights_lab_fe, weights_H_type)\n",
    "# for key in lt_stats_no_wH:\n",
    "#     print(key, lt_stats_no_wH[key])\n",
    "\n",
    "# print()\n",
    "\n",
    "earnings_H_good = log_lab_earn_H_good\n",
    "weights_lab_fe = myPars_H_good.lab_fe_weights\n",
    "weights_H_type = myPars_H_good.H_type_perm_weights\n",
    "lt_stats_H_good = calculate_weighted_lt_stats(earnings_H_good, weights_lab_fe, weights_H_type)\n",
    "for key in lt_stats_H_good:\n",
    "    print(key, lt_stats_H_good[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_earn: 0.0898328343083297\n",
      "var_earn_H: 0.08946066588319593\n",
      "liftetime earn var %: 0.4160134752626235\n",
      "log lifetime earn var %: 0.4151505316844606\n",
      "raw diff: 0.0003721684251337676\n"
     ]
    }
   ],
   "source": [
    "var_earn = lt_stats[\"var\"]\n",
    "var_earn_H = lt_stats_H_good[\"var\"]\n",
    "print(f\"var_earn: {var_earn}\")\n",
    "print(f\"var_earn_H: {var_earn_H}\")\n",
    "print(f\"liftetime earn var %: {((var_earn - var_earn_H)/var_earn_H)*100}\")\n",
    "print(f\"log lifetime earn var %: {(np.log(var_earn) - np.log(var_earn_H))*100}\")\n",
    "print(f'raw diff: {var_earn - var_earn_H}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
